---
title: "P8105 HW 3"
author: "AJ Catalano"
date: "11/10/2021"
output: github_document
---

```{r}
library(tidyverse)
library(p8105.datasets)
data("instacart")

knitr::opts_chunk$set(
  fig.width = 8,
  fig.height = 6,
  out.width = "80%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.color = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

The instacart dataset contains `r nrow(instacart)` observations and `r ncol(instacart)` variables. It contains information about Instacart orders, including order numbers, product IDs, product names, and product location (department and aisles). For example, someone ordered Lightly Smoked Sardines in Olive Oil, a product that is located in the "canned meat seafood" aisle of the "canned goods" department.

```{r}
# determining number of aisles:

instacart %>%
  summarize(aisles_tot = n_distinct(aisle))
```

The number of unique aisles from this dataset is `r instacart %>% summarize(aisles_tot = n_distinct(aisle))`.

The 10 most popular aisles are provided in the table below:

```{r echo = FALSE}
# determining the 10 most popular aisles:

instacart %>% 
  mutate(
    aisle = as.factor(aisle)) %>%
  count(aisle) %>%
  rename(aisle_freq = n) %>% 
  arrange(desc(aisle_freq)) %>% 
  head(10) %>% 
  knitr::kable()
```

Below is a plot showing the number of items ordered in each aisle for aisles with > 10000 items ordered.

```{r}
instacart %>% 
  count(aisle) %>%
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  labs(title = "Purchases by Aisle",
       x = "Purchases",
       y = "Aisle") + 
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

A table showing the three most popular items (by number of purchases) for `packaged vegetables fruits`, `baking ingredients`, and `dog food care` is shown below:

```{r}
# why doesn't aisle == c() give correct table (n is lower, rank is correct)

instacart %>% 
  filter(aisle %in% c("baking ingredients", 
                    "dog food care", 
                    "packaged vegetables fruits")) %>% 
  group_by(aisle) %>%
  count(product_name) %>% 
  arrange(desc(n)) %>% 
  mutate(
    rank = min_rank(desc(n))) %>%
  filter(rank <= 3) %>% 
  knitr::kable()
```

Below is a table showing the mean hour of the day that coffee ice cream and pink lady apples are ordered for each day of the week.

```{r}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  select(order_dow, product_name, order_hour_of_day) %>% 
  group_by(order_dow, product_name) %>% 
  summarize(
    mean_hour = mean(order_hour_of_day)
    ) %>%
  pivot_wider(names_from = order_dow, values_from = mean_hour) %>% 
  knitr::kable()
```


## Problem 2

format the data to use appropriate variable names;
focus on the “Overall Health” topic
include only responses from “Excellent” to “Poor”
organize responses as a factor taking levels ordered from “Poor” to “Excellent”

```{r}
data("brfss_smart2010")

brfss_overall_health = 
  brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  rename(
    state = locationabbr,
    county = locationdesc,
    ) %>%
  filter(
    topic == "Overall Health",
    response %in% 
      c("Excellent", "Very good", "Good", "Fair", "Poor")
    ) %>%
  mutate(
    response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent")))
```

```{r eval = FALSE}
# code to show all unique values for a given variable

unique(brfss_smart2010[c("Response")])
```

In 2002, 6 states were observed at 7 or more locations. In 2010, 14 states were observed at 7 or more locations.

```{r}
brfss_overall_health %>% 
  filter(year %in% c(2002, 2010)) %>% 
  select(state, county, year) %>% 
  group_by(year, state) %>% 
  summarize(
    locations = n_distinct(county)
  ) %>% 
  filter(
    locations >= 7
  ) %>%
  pivot_wider(
    names_from = state,
    values_from = locations
  ) %>% 
  knitr::kable()
```


Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).

```{r}
brfss_overall_health %>% 
  filter(response == "Excellent") %>% 
  group_by(year, state) %>% 
  summarize(
    mean_data_value = mean(data_value)
  ) %>% 
  ggplot(aes(x = year, y = mean_data_value, color = state)) +
  geom_line() +
  labs(
    title = "Mean Data Value v. Year",
    x = "Year",
    y = "Mean Data Value"
  ) +
  theme_minimal() +
  theme(legend.position = "none")
  
```

Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

```{r}
brfss_overall_health %>% 
  filter(
    year %in% c(2006, 2010),
    response %in% c("Excellent", "Very good", "Good", "Fair", "Poor"),
    state == "NY"
  ) %>% 
  ggplot(aes(x = response, y = data_value)) +
  geom_boxplot() +
  facet_grid(. ~ year) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

## Problem 3

The cleaned dataset includes `r nrow(accel_data_clean)` observations and `r ncol(accel_data_clean)` variables, including week, day, and activity count.

```{r}
accel_data =
  read_csv("./accel_data.csv")

accel_data_clean =
  accel_data %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "activity_count"
  ) %>% 
  mutate(
    day = factor(day),
    day = fct_relevel(day, "Sunday", "Monday", "Tuesday", "Wednesday",
                                 "Thursday", "Friday", "Saturday"),
    wkend_wkday = as.numeric(day %in% c("Saturday", "Sunday")),
    wkend_wkday = recode(wkend_wkday, `0` = "weekday", `1` = "weekend"),
    minute = as.numeric(minute)
  )
```

Obvious trends are difficult to determine from the table below, but it is noteworthy that on Saturdays from weeks 4 and 5, there was an unusually low total activity count. As both totals are 1440, it is likely that each minute's activity was coded as `1`.

```{r}
accel_data_clean %>% 
  group_by(week, day) %>% 
  summarize(
    tot_activity = sum(activity_count)
  ) %>% 
  pivot_wider(
    names_from = day,
    values_from = tot_activity
  ) %>% 
  knitr::kable()
```

Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

```{r}
accel_data_clean %>% 
  ggplot(aes(x = minute,
             y = activity_count,
             group = day_id,
             color = day)) +
  geom_line(alpha = 0.2) +
  geom_smooth(aes(group = day), alpha = 0.3, se = FALSE)
```

